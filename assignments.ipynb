{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use iris dataset to write an MLflow program with below requirements to classify the 'variety' of petal.\n",
    "\n",
    "\n",
    "\n",
    "1. Experiments, Runs, Parameters, Metrics shall be logged and tracked.\n",
    "2. Input dataset, Train and Test data, Model shall be logged as artifacts.\n",
    "3. Set multiple tags to the run.\n",
    "4. Print the evaluation metrics.\n",
    "5. Print the last active run.\n",
    "\n",
    "Questions for this assignment\n",
    "Which library, module and class can be used for this problem statement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import warnings, os, joblib\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics, svm\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(40)\n",
    "    \n",
    "    \n",
    "    # Set the experiment\n",
    "    exp = mlflow.set_experiment(experiment_name=\"assignment_1\")\n",
    "\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # Encode the classes to codes and load the dataset, change the variety data from categorical to numerical codes\n",
    "    class_codes = {\"Setosa\": 1, \"Versicolor\": 2, \"Virginica\": 3}\n",
    "    data =  pd.read_csv(\"data/iris.csv\")\n",
    "    actual_data = data.replace({\"variety\": class_codes})\n",
    "    features = [\"sepal.length\", \"sepal.width\", \"petal.length\", \"petal.width\"]\n",
    "    features = actual_data[features]\n",
    "    labels =  actual_data[\"variety\"]\n",
    "    \n",
    "    # Create the train-test split\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(features.to_numpy(), labels.to_numpy(), random_state=1)\n",
    "    print(f\"The number of data rows in the dataset is: {len(actual_data)} and after the train-test split we have {len(train_data)} train rows and {len(test_data)} test rows\")\n",
    "    \n",
    "    \n",
    "    # Log the input dataset, train and test datasets\n",
    "    mlflow.log_artifact(\"data/iris.csv\", \"data\")\n",
    "    \n",
    "    train_df = pd.DataFrame(train_data, columns=[\"sepal.length\", \"sepal.width\", \"petal.length\", \"petal.width\"])\n",
    "    train_df.to_csv(\"train.csv\")\n",
    "    mlflow.log_artifact(\"train.csv\", \"data\")\n",
    "    os.remove(\"train.csv\")\n",
    "\n",
    "    test_df  = pd.DataFrame(test_data, columns=[\"sepal.length\", \"sepal.width\", \"petal.length\", \"petal.width\"])\n",
    "    train_df.to_csv(\"test.csv\")\n",
    "    mlflow.log_artifact(\"test.csv\", \"data\")\n",
    "    os.remove(\"test.csv\")\n",
    "    \n",
    "    # Create all the classifiers we want to test on the data and save as well [Decision Tree classifier object, Support Vector Machine]\n",
    "    decision_tree_classifier = DecisionTreeClassifier()\n",
    "    svm_classifier = svm.SVC(kernel='linear')\n",
    "    \n",
    "    # Train the models\n",
    "    decision_tree_classifier = decision_tree_classifier.fit(train_data, train_labels)\n",
    "    svm_classifier = svm_classifier.fit(train_data, train_labels)\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    decision_tree_predictions = decision_tree_classifier.predict(test_data)\n",
    "    svm_predictions = svm_classifier.predict(test_data)\n",
    "    print(\"Decision Tree classifier Accuracy:\",metrics.accuracy_score(test_labels, decision_tree_predictions))\n",
    "    print(\"Support Vector Machine Accuracy:\",metrics.accuracy_score(test_labels, decision_tree_predictions))\n",
    "    \n",
    "    # Save the two models and save them in an artifact\n",
    "    joblib.dump(decision_tree_classifier, \"decision_tree_classifier.sav\")\n",
    "    joblib.dump(svm_classifier, \"svm_classifier.sav\")\n",
    "    mlflow.log_artifact(\"decision_tree_classifier.sav\", \"model\")\n",
    "    mlflow.log_artifact(\"svm_classifier.sav\", \"model\")\n",
    "    os.remove(\"decision_tree_classifier.sav\")\n",
    "    os.remove(\"svm_classifier.sav\")\n",
    "    \n",
    "    \n",
    "    tags = {\"Model\": \"Decision Tree\", \"Model\": \"Support Vector Machine\"}\n",
    "    mlflow.set_tags(tags=tags)\n",
    "    \n",
    "    mlflow.end_run()\n",
    "    print(f\"The last active run is: {mlflow.last_active_run()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and validate the Model created in Assignment 1 (classification with iris dataset) with a DummyClassifier baseline model using mlflow.evaluate() method. Solution to Assignment 1 is attached.\n",
    "\n",
    "Keep the accuracy threshold to 0.8 in evaluate method\n",
    "\n",
    "Create 4 custom evaluation metrics using the below functions from sklearn.metrics module on predicted and actual target values\n",
    "\n",
    "- accuracy_score\n",
    "\n",
    "- precision_score\n",
    "\n",
    "- recall_score\n",
    "\n",
    "- f1_score\n",
    "\n",
    "Note : Metrics should be computed by taking the weighted average across classes\n",
    "\n",
    "Questions for this assignment\n",
    "Which argument would you use while defining custom metrics methods ?\n",
    "\n",
    "_builtin_metrics OR  builtin_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import MetricThreshold\n",
    "from mlflow.models import make_metric\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--penality\", type=str, required=False, default=\"l2\")\n",
    "parser.add_argument(\"--C\", type=float, required=False, default=1.0)\n",
    "args = parser.parse_args()\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    accuracy = accuracy_score(actual, pred)\n",
    "    precision = precision_score(actual, pred, average='weighted')\n",
    "    recall = recall_score(actual, pred, average='weighted')\n",
    "    f1 = f1_score(actual, pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(40)\n",
    "\n",
    "    # Read the dataset\n",
    "    data = pd.read_csv('data/iris.csv')\n",
    "    # Split the data into training and test sets.\n",
    "    train, test = train_test_split(data)\n",
    "\n",
    "    # Data Preprocessing\n",
    "    le = LabelEncoder()\n",
    "    train['variety'] = le.fit_transform(train['variety'])\n",
    "    test['variety'] = le.fit_transform(test['variety'])\n",
    "\n",
    "    # Storing the training and testing dataset\n",
    "    train.to_csv(\"data/train.csv\")\n",
    "    test.to_csv(\"data/test.csv\")\n",
    "\n",
    "    # Split\n",
    "    train_x = train.drop([\"variety\"], axis=1)\n",
    "    test_x = test.drop([\"variety\"], axis=1)\n",
    "    train_y = train[[\"variety\"]]\n",
    "    test_y = test[[\"variety\"]]\n",
    "\n",
    "    # Hyperparameters\n",
    "    penality = args.penality\n",
    "    C = args.C\n",
    "\n",
    "    experiment = mlflow.set_experiment(\n",
    "        experiment_name=\"Classifier exp\"\n",
    "    )\n",
    "\n",
    "    print(\"Name: {}\".format(experiment.name))\n",
    "    print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "    print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "    print(\"Tags: {}\".format(experiment.tags))\n",
    "    print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\n",
    "    print(\"Creation timestamp: {}\".format(experiment.creation_time))\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Runer6\", experiment_id=experiment.experiment_id):\n",
    "        tags = {\n",
    "            \"engineering\": \"ML platform\",\n",
    "            \"release.candidate\": \"RC1\",\n",
    "            \"release.version\": \"2.0\"\n",
    "        }\n",
    "        #set tags\n",
    "        mlflow.set_tags(tags)\n",
    "\n",
    "        # Model\n",
    "        lr = LogisticRegression(penalty=penality, C=C)\n",
    "        lr.fit(train_x, train_y)\n",
    "\n",
    "        predicted_classes = lr.predict(test_x)\n",
    "\n",
    "        (accuracy, precision, recall, f1) = eval_metrics(test_y, predicted_classes)\n",
    "\n",
    "        print(f\"Logistic Regression model (penality={penality}, C={C}):\")\n",
    "        print(\"  Accuracy: %s\" % accuracy)\n",
    "        print(\"  Precision: %s\" % precision)\n",
    "        print(\"  Recall: %s\" % recall)\n",
    "        print(\"  F1 Score: %s\" % f1)\n",
    "\n",
    "        # Logging parameters\n",
    "        params = {\n",
    "            \"penality\": penality,\n",
    "            \"C\": C\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Logging Metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1\n",
    "        }\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # Logging artifacts the data\n",
    "        mlflow.log_artifacts(\"data/\")\n",
    "        # Logging model\n",
    "        mlflow.sklearn.log_model(lr, \"model\")\n",
    "        \n",
    "        \n",
    "        # Step 1: Create the dummy classifier and fit on data\n",
    "        dummy_classifier = DummyClassifier(strategy=\"uniform\").fit(train_x, train_y)\n",
    "        \n",
    "        # Step 2: Create the predicted classes in order to get the accuracy metrics\n",
    "        predicted_classes = lr.predict(test_x)\n",
    "        (accuracy, precision, recall, f1) = eval_metrics(test_y, predicted_classes)\n",
    "        print(\" For the Dummy Classifier the accuracy metrics are the following:  \")\n",
    "        print(f\"Logistic Regression model (penality={penality}, C={C}):\")\n",
    "        print(\"  Accuracy: %s\" % accuracy)\n",
    "        print(\"  Precision: %s\" % precision)\n",
    "        print(\"  Recall: %s\" % recall)\n",
    "        print(\"  F1 Score: %s\" % f1)\n",
    "        \n",
    "        \n",
    "        thresholds = {\n",
    "        \"accuracy_score\": MetricThreshold(\n",
    "            threshold=0.8,  # accuracy should be >=0.8\n",
    "            greater_is_better=True, ),\n",
    "        }\n",
    "        \n",
    "        # Define custom metric functions\n",
    "        def weighted_accuracy_fn(eval_df, _builtin_metrics):\n",
    "            y_true = eval_df[\"target\"]\n",
    "            y_pred = eval_df[\"prediction\"]\n",
    "            return accuracy_score(y_true, y_pred)\n",
    "\n",
    "        def weighted_precision_fn(eval_df, _builtin_metrics):\n",
    "            y_true = eval_df[\"target\"]\n",
    "            y_pred = eval_df[\"prediction\"]\n",
    "            return precision_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "        def weighted_recall_fn(eval_df, _builtin_metrics):\n",
    "            y_true = eval_df[\"target\"]\n",
    "            y_pred = eval_df[\"prediction\"]\n",
    "            return recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "        def weighted_f1_fn(eval_df, _builtin_metrics):\n",
    "            y_true = eval_df[\"target\"]\n",
    "            y_pred = eval_df[\"prediction\"]\n",
    "            return f1_score(y_true, y_pred, average=\"weighted\")\n",
    "        \n",
    "        \n",
    "        weighted_accuracy_fn_metric = make_metric(\n",
    "            eval_fn=weighted_accuracy_fn,\n",
    "            greater_is_better=False,\n",
    "            name=\"weighted accuracy\"\n",
    "        )\n",
    "                \n",
    "        weighted_precision_fn_metric = make_metric(\n",
    "            eval_fn=weighted_precision_fn,\n",
    "            greater_is_better=False,\n",
    "            name=\"weighted precision\"\n",
    "        )\n",
    "\n",
    "        weighted_recall_fn_metric = make_metric(\n",
    "            eval_fn=weighted_recall_fn,\n",
    "            greater_is_better=False,\n",
    "            name=\"weighted recall\"\n",
    "        )\n",
    "            \n",
    "        weighted_f1_fn_metric = make_metric(\n",
    "            eval_fn=weighted_f1_fn,\n",
    "            greater_is_better=False,\n",
    "            name=\"weighted f1\"\n",
    "        )\n",
    "        \n",
    "\n",
    "        \n",
    "        candidate_model_uri = mlflow.sklearn.log_model(candidate_model, \"candidate_model\", signature=signature).model_uri\n",
    "        baseline_model_uri = mlflow.sklearn.log_model(baseline_model, \"baseline_model\", signature=signature).model_uri\n",
    "        \n",
    "        mlflow.evaluate(candidate_model_uri, eval_data, targets=\"label\", model_type=\"classifier\", validation_thresholds=thresholds, baseline_model=baseline_model_uri,)\n",
    "\n",
    "        \n",
    "\n",
    "        run = mlflow.last_active_run()\n",
    "        print(\"Active run_id: {}\".format(run.info.run_id))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
